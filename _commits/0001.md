---
layout: post
date: 2016-09-12
title: "Managing Complexity In Databases"
description: |
  This is the first commit.
keywords:
  - demo blog
  - blog 
  - programming 
excerpt: |
  Databse and JSON
---


Let's draw a picture: new project kicked off. There is a clear vision and code. But during time all projects tend to take on a life of their own, growing as a web of features, requirements, and code modules. This growth, while a sign of a project's success, also brings with it an increase in complexity that, if not managed properly, can become overwhelming. I believe you already recognize this story. 
Here, actual programming begins (LOL), as all we know, to steer this complexity into a form that is maintainable and scalable. We need to ensure that the expansion of our project is linear or at least predictable in its complexity. I can confidently say, a prime battleground for this effort is often the project's relational database schema. 

Relational databases have been the bedrock of software systems for decades and their story is far from end. 
It is like to a well-oiled machine, offering a structured and dependable method to store and retrieve the data: blood of any applications. 

You already noticed, we start talking about complexity. The sad story is next the once clear schemas tend to expand into intricate networks of tables and relationships. And surprise, well-normalized database can evolve into a monolith that is cumbersome to query and expensive to maintain:
- Increase the complexity of the SQL queries.
- Join operations becomes so numerous and intricate that they impact performance and slow down development
- Applications require magical ORM frameworks to manage data. But, all of we, I hope, not likes a software magic.
- DB performance overall
- More tables are more fun (sarcasm: it is more difficult) to replicate to secondary servers


This topic can be really hot for example at payment systems. But what to do?  Here's a good trick: we can start combining NoSQL approaches for SQL databases.


### Solution 

Firstly, we need to define boundaries in our domain clearly. Picture a pyament system, responsible for facilitating a transactions, the foremost boundary context is the 'payment' module. This  segment has approximately ten distinct entities to collectively encapsulate an *order*. Anyone who has noticed the spirit of DDD in all this can praise themselves, this is really so.

Secondly, if we honestly write this down in a normalized form, we get about 3-4 dozen tables. It becomes sad to work with this, and changing the database schema is very sad. What we can do here? A pragmatic solution for a highly normalized database structure is to utilize JSON.

```sql
create table orders(
	id uuid not null primary key,
	external_ref varchar(32) not null,
	state varchar(25) not null,
	revision bigint not null,
	schema bigint not null,
	data json not null
);
```

At first glance, this might resemble a NoSQL approach, but it's rooted in a relational database system. I would like to breakdoen it as ChatGPT likes to do:

- **JSON:** In application we use any nasty serialization/deserialization framework to handle json. There is a lot of them. In the java world there is Jackson who can deal with polymorphism and a lot of different tricks and options. 
- **Surprise, surprise:** Databases today already support JSON type on native level. At least for normal databases, like Postgres we have ::json and ::jsonb. 
- **No ORM magic:** We do not need any magical ORM for such simple table structures. There is no magic here; you will not find normal payment systems with ORM!
- **Transactions and Locks:** Even though the table stores domain entities as a JSON blob, it remains a table, ensuring the benefits of ACID properties and other relational database features.
- **Consistency:** The system still offers all the consistency guarantees that a standard relational database provides. Today there are still almost no systems that can compete in this with relational databases.

To efficiently work with a JSON-based storage model, it's imperative to establish a set of rules and best practices. These might include:

**Indexes** Implementing indexes on JSON fields where necessary to maintain performance. Fields duplicated as a column in table, where normal db index is created. This approach combines the flexibility of JSON with record searchability. Moreover, we continue to think in the paradigm of relations and tables, we simply take the table as Aggregate Root from DDD design. 

In my example: table `operations` has column `external_ref` as a demostration of this pattern. You can imagin scenarion that we ususaly want to find order by exteranl reference when modifing it. 

**Version** This is important to store a scheam of the json structure as separate column. In my example: table `operations` has column `schema`. The aim pritty simple, keep structure of JSON under control. Struture of you object definitly will  change. And sometimes, without backward compatibility. So, prepare in advance simple tools for transferring this material, it will definitely come in handy. 

**Optimistic Locking For Free** Yes, such a scheme can have optimistic locking for free. Can you imagine how to add such a lock on 30 tables? Never. 

In my example: table `operations` has column `revistion` as a demostration of this pattern.



**Benefits:**

1. **Flexibility:** This approach offers the flexibility of NoSQL databases while retaining the robustness of relational databases.
2. **Scalability:** It allows for easier scaling as the project grows, without the complexities of traditional normalization.
3. **Clear Boundaries:** By defining bounded contexts, it ensures that different systems or modules remain decoupled, promoting modular and maintainable code.

In conclusion, as projects grow, it's essential to adopt strategies that manage complexity effectively. By combining the strengths of both relational and NoSQL databases, developers can ensure that their projects remain scalable, maintainable, and efficient.